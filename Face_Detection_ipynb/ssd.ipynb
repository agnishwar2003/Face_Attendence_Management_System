{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import imutils\n",
    "import cv2\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the paths to the prototxt file and pre-trained model\n",
    "prototxt_path = r\"D:\\PythonProject\\Face_Recognition_DL\\model\\deploy.prototxt.txt\"\n",
    "model_path = r\"D:\\PythonProject\\Face_Recognition_DL\\model\\res10_300x300_ssd_iter_140000.caffemodel\"\n",
    "confidence_threshold = 0.5\n",
    "\n",
    "# Resize OpenCV window size\n",
    "cv2.namedWindow(\"Frame\", cv2.WINDOW_NORMAL)\n",
    "\n",
    "# Load model architecture and weights\n",
    "print(\"[INFO] loading model...\")\n",
    "net = cv2.dnn.readNetFromCaffe(prototxt_path, model_path)\n",
    "\n",
    "# Initialize camera stream\n",
    "print(\"[INFO] starting video stream...\")\n",
    "\n",
    "vid = cv2.VideoCapture(0)\n",
    "\n",
    "# Loop over the frames from the video stream\n",
    "while True:\n",
    "    # Start the timer to calculate inference time\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Read frame from camera and resize to 400 pixels\n",
    "    ret, frame = vid.read()\n",
    "    frame = imutils.resize(frame, width=400)\n",
    " \n",
    "    # Grab the frame dimensions and convert it to a blob\n",
    "    (h, w) = frame.shape[:2]\n",
    "    blob = cv2.dnn.blobFromImage(cv2.resize(frame, (300, 300)), 1.0,\n",
    "                                 (300, 300), (104.0, 177.0, 123.0))\n",
    " \n",
    "    # Pass the blob through the network and obtain the detections and predictions\n",
    "    net.setInput(blob)\n",
    "    detections = net.forward()\n",
    "\n",
    "    # Calculate inference time\n",
    "    inference_time = time.time() - start_time\n",
    "\n",
    "    # Add inference time text on top of the frame\n",
    "    cv2.putText(frame, \"Inference Time: {:.4f} sec\".format(inference_time), (10, 30),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.3, (0, 255, 0), 1)\n",
    "\n",
    "    # Loop over the detections\n",
    "    for i in range(0, detections.shape[2]):\n",
    "        # Extract the confidence (i.e., probability) associated with the prediction\n",
    "        confidence = detections[0, 0, i, 2]\n",
    "\n",
    "        # Filter out detections by confidence\n",
    "        if confidence < confidence_threshold:\n",
    "            continue\n",
    "\n",
    "        # Compute the (x, y)-coordinates of the bounding box for the object\n",
    "        box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "        (startX, startY, endX, endY) = box.astype(\"int\")\n",
    " \n",
    "        # Draw the bounding box of the face along with the associated probability\n",
    "        text = \"{:.2f}%\".format(confidence * 100)\n",
    "        y = startY - 10 if startY - 10 > 10 else startY + 10\n",
    "        cv2.rectangle(frame, (startX, startY), (endX, endY),\n",
    "                      (0, 0, 255), 1)\n",
    "        cv2.putText(frame, text, (startX, y),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.3, (0, 0, 255), 1)\n",
    "\n",
    "    # Show the output frame\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "    # If the `q` key was pressed, break from the loop\n",
    "    if key == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "# After the loop, release the cap object and destroy all windows\n",
    "vid.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Define the paths to the prototxt file and pre-trained model\n",
    "prototxt_path = r\"D:\\PythonProject\\Face_Recognition_DL\\model\\deploy.prototxt.txt\"\n",
    "model_path = r\"D:\\PythonProject\\Face_Recognition_DL\\model\\res10_300x300_ssd_iter_140000.caffemodel\"\n",
    "confidence_threshold = 0.5\n",
    "\n",
    "# Load model architecture and weights\n",
    "print(\"[INFO] loading model...\")\n",
    "net = cv2.dnn.readNetFromCaffe(prototxt_path, model_path)\n",
    "\n",
    "# Load the image\n",
    "image_path = r\"D:\\PythonProject\\Face_Recognition_DL\\Multiple_selfie.jpg\"  # Change this to your image path\n",
    "image = cv2.imread(image_path)\n",
    "image = cv2.resize(image, (600, 500))\n",
    "\n",
    "# Grab the image dimensions\n",
    "(h, w) = image.shape[:2]\n",
    "\n",
    "# Convert image to a blob\n",
    "blob = cv2.dnn.blobFromImage(cv2.resize(image, (300, 300)), 1.0,\n",
    "                             (300, 300), (104.0, 177.0, 123.0))\n",
    "\n",
    "# Pass the blob through the network and obtain the detections\n",
    "net.setInput(blob)\n",
    "detections = net.forward()\n",
    "\n",
    "# Loop over the detections\n",
    "for i in range(0, detections.shape[2]):\n",
    "    # Extract the confidence associated with the prediction\n",
    "    confidence = detections[0, 0, i, 2]\n",
    "\n",
    "    # Filter out detections by confidence\n",
    "    if confidence < confidence_threshold:\n",
    "        continue\n",
    "\n",
    "    # Compute the (x, y)-coordinates of the bounding box\n",
    "    box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "    (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "\n",
    "    # Draw the bounding box and probability\n",
    "    text = \"{:.2f}%\".format(confidence * 100)\n",
    "    y = startY - 10 if startY - 10 > 10 else startY + 10\n",
    "    cv2.rectangle(image, (startX, startY), (endX, endY),\n",
    "                  (0, 0, 255), 1)\n",
    "    cv2.putText(image, text, (startX, y),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1)\n",
    "\n",
    "# Show the output image\n",
    "cv2.imshow(\"Face Detection\", image)\n",
    "cv2.imwrite(r\"D:\\PythonProject\\Face_Recognition_DL\\Face_detection_test_op\\SSD.jpg\", image)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading model...\n",
      "[INFO] Starting video stream...\n",
      "Starting capture for batch 1\n",
      "Press 'c' to capture the next batch of images, or 'q' to stop.\n",
      "Starting capture for batch 2\n",
      "Press 'c' to capture the next batch of images, or 'q' to stop.\n",
      "Starting capture for batch 3\n",
      "Press 'c' to capture the next batch of images, or 'q' to stop.\n",
      "Starting capture for batch 4\n",
      "Press 'c' to capture the next batch of images, or 'q' to stop.\n",
      "Starting capture for batch 5\n",
      "Press 'c' to capture the next batch of images, or 'q' to stop.\n",
      "[INFO] Captured 500 images, saved in 'D:\\PythonProject\\Face_Recognition_DL\\Agnishwar_Test_Dataset' directory.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Define paths to model files\n",
    "prototxt_path = r\"D:\\PythonProject\\Face_Recognition_DL\\model\\deploy.prototxt.txt\"\n",
    "model_path = r\"D:\\PythonProject\\Face_Recognition_DL\\model\\res10_300x300_ssd_iter_140000.caffemodel\"\n",
    "\n",
    "# Confidence threshold for face detection\n",
    "confidence_threshold = 0.5\n",
    "\n",
    "# Create a directory to save images if it doesn't exist\n",
    "output_dir = r\"D:\\PythonProject\\Face_Recognition_DL\\Agnishwar_Test_Dataset\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Load the Caffe face detection model\n",
    "print(\"[INFO] Loading model...\")\n",
    "net = cv2.dnn.readNetFromCaffe(prototxt_path, model_path)\n",
    "\n",
    "# Initialize webcam\n",
    "print(\"[INFO] Starting video stream...\")\n",
    "vid = cv2.VideoCapture(0)\n",
    "\n",
    "# Image capture settings\n",
    "img_count = 0\n",
    "batch_size = 100\n",
    "total_images = 500\n",
    "\n",
    "while img_count < total_images:\n",
    "    captured_this_batch = 0\n",
    "    print(f\"Starting capture for batch {img_count // batch_size + 1}\")\n",
    "\n",
    "    while captured_this_batch < batch_size and img_count < total_images:\n",
    "        ret, frame = vid.read()\n",
    "        if not ret:\n",
    "            print(\"Failed to grab frame\")\n",
    "            break\n",
    "\n",
    "        # Resize frame for better processing\n",
    "        frame = cv2.resize(frame, (600, 500))\n",
    "        (h, w) = frame.shape[:2]\n",
    "\n",
    "        # Preprocess frame for the face detection model\n",
    "        blob = cv2.dnn.blobFromImage(cv2.resize(frame, (300, 300)), 1.0, (300, 300), (104.0, 177.0, 123.0))\n",
    "        net.setInput(blob)\n",
    "        detections = net.forward()\n",
    "\n",
    "        for i in range(detections.shape[2]):\n",
    "            confidence = detections[0, 0, i, 2]\n",
    "            if confidence < confidence_threshold:\n",
    "                continue\n",
    "\n",
    "            # Get bounding box coordinates\n",
    "            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "            (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "\n",
    "            # Crop and validate face detection\n",
    "            face_cropped = frame[startY:endY, startX:endX]\n",
    "            if face_cropped.shape[0] < 10 or face_cropped.shape[1] < 10:\n",
    "                continue  # Ignore very small detections\n",
    "\n",
    "            # Resize to 96x96 for OpenFace compatibility\n",
    "            face_resized = cv2.resize(face_cropped, (96, 96))\n",
    "\n",
    "            # Convert to grayscale\n",
    "            face_gray = cv2.cvtColor(face_resized, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            # Save the grayscale face image\n",
    "            img_filename = os.path.join(output_dir, f\"face_{img_count+1}.jpg\")\n",
    "            cv2.imwrite(img_filename, face_gray)\n",
    "            img_count += 1\n",
    "            captured_this_batch += 1\n",
    "\n",
    "            # Draw bounding box on frame\n",
    "            cv2.rectangle(frame, (startX, startY), (endX, endY), (0, 255, 0), 1)\n",
    "            cv2.putText(frame, f\"Captured: {img_count}/{total_images}\", (10, 30),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
    "\n",
    "            # Stop capturing when batch is complete\n",
    "            if captured_this_batch >= batch_size or img_count >= total_images:\n",
    "                break\n",
    "\n",
    "        # Show the live video feed\n",
    "        cv2.imshow(\"Face Capture\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "    print(\"Press 'c' to capture the next batch of images, or 'q' to stop.\")\n",
    "    key = cv2.waitKey(0) & 0xFF\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "vid.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(f\"[INFO] Captured {img_count} images, saved in '{output_dir}' directory.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HogEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
